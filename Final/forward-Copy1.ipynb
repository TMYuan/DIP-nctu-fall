{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from model_old import PSPModule, PSPNet, PSPUpsample\n",
    "import numpy as np\n",
    "from skimage import io, measure, transform\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "width= 5000\n",
    "high= 5000\n",
    "edge_size= 512\n",
    "crop_size= 512\n",
    "confident_size= 400\n",
    "USE_GPU = torch.cuda.is_available()\n",
    "input_path= './input2/'\n",
    "save_path= './save/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function Definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Test Image Cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def crop(image, crop_size, confident_size):\n",
    "    \n",
    "    edge_size= 512\n",
    "    \n",
    "    high= image.shape[0]\n",
    "    width= image.shape[1]\n",
    "    depth= image.shape[2]\n",
    "    assert(high, width)== (5000, 5000)\n",
    "    \n",
    "    atoll_size= (crop_size- confident_size)// 2\n",
    "    crop_list= []\n",
    "    \n",
    "    for col in range(edge_size, width, edge_size):\n",
    "        crop_list.append(image[0: edge_size, col- edge_size: col, :])\n",
    "    crop_list.append(image[0: 0+ crop_size, -edge_size:, :])\n",
    "    for row in range(edge_size* 2, high, edge_size):\n",
    "        crop_list.append(image[row- edge_size: row, 0: edge_size, :])\n",
    "        crop_list.append(image[row- edge_size: row, -edge_size:, :])\n",
    "    for col in range(edge_size, width, edge_size):\n",
    "        crop_list.append(image[-edge_size:, col- edge_size: col, :])\n",
    "    crop_list.append(image[-edge_size:, -edge_size:, :])\n",
    "    \n",
    "    num_row= (high- 2* atoll_size)// confident_size\n",
    "    num_col= (width- 2* atoll_size)// confident_size\n",
    "    \n",
    "    for row in range(num_row):\n",
    "        for col in range(num_col):\n",
    "            crop_list.append(image[row* confident_size: (row+ 1)* confident_size+ 2* atoll_size,\n",
    "                                   col* confident_size: (col+ 1)* confident_size+ 2* atoll_size,\n",
    "                                   :])\n",
    "    return crop_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Numpy Array to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def toTensor(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    image= transform(image)\n",
    "    image= image.unsqueeze(0)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def combine(crop_list, crop_size, confident_size, high= 5000, width= 5000):\n",
    "    \n",
    "    edge_size= 512\n",
    "    \n",
    "    comb= np.empty([width, high])\n",
    "    atoll_size= (crop_size- confident_size)// 2 \n",
    "    count= 0\n",
    "\n",
    "    for col in range(edge_size, width, edge_size):\n",
    "        comb[0: edge_size, col- edge_size: col]= lbl_list[count]\n",
    "        count+= 1\n",
    "    comb[0: 0+ crop_size, -edge_size:]= lbl_list[count]\n",
    "    count+= 1\n",
    "    for row in range(edge_size* 2, high, edge_size):\n",
    "        comb[row- edge_size: row, 0: edge_size]= lbl_list[count]\n",
    "        comb[row- edge_size: row, -edge_size:]= lbl_list[count+ 1]\n",
    "        count+= 2\n",
    "    for col in range(edge_size, width, edge_size):\n",
    "        comb[-edge_size:, col- edge_size: col]= lbl_list[count]\n",
    "        count+= 1\n",
    "    comb[-edge_size:, -edge_size:]= lbl_list[count]\n",
    "    count+= 1\n",
    "\n",
    "    for row in range(atoll_size+ confident_size, high, confident_size):\n",
    "        for col in range(atoll_size+ confident_size, width, confident_size):\n",
    "            comb[row- confident_size: row,\n",
    "                 col- confident_size: col]= lbl_list[count][atoll_size: confident_size+ atoll_size, \n",
    "                                                            atoll_size: confident_size+ atoll_size]\n",
    "            count+= 1\n",
    "\n",
    "    return comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forwarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/anaconda3/envs/pytorch/lib/python3.6/site-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "test_list= os.listdir(input_path)\n",
    "\n",
    "# model = torch.load('./model-0113-test-40ep-0.9607.pt')\n",
    "model = torch.load('./model-0.9548.pt')\n",
    "\n",
    "im_name= 'input5.tif'\n",
    "\n",
    "image= io.imread(input_path+ im_name)\n",
    "image= transform.resize(image, [512, 512, 3])\n",
    "\n",
    "image*= 255\n",
    "test= toTensor(image.astype('uint8'))\n",
    "\n",
    "if USE_GPU:\n",
    "    test= Variable(test.cuda())\n",
    "else:\n",
    "    test= Variable(test)\n",
    "\n",
    "model.eval()\n",
    "output= model(test)\n",
    "output= transforms.ToPILImage()(output.data.cpu()[0])\n",
    "output= np.array(output)\n",
    "\n",
    "comb= output\n",
    "comb[comb>= 127]= 255\n",
    "comb[comb< 127]= 0\n",
    "label, count= measure.label(comb, return_num= True)\n",
    "\n",
    "comb= transform.resize(comb, [5000, 5000])\n",
    "comb*= 255\n",
    "\n",
    "io.imsave(save_path+ im_name, comb.astype('uint8'))\n",
    "    \n",
    "\n",
    "with open(save_path+ 'count.txt', 'a') as f:\n",
    "    f.write('{}:{}\\n'.format(im_name, count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
